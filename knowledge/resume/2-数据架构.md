## 资料

https://blog.csdn.net/qq_26803795/category_9271568.html

[美团外卖实时数仓建设实践](https://tech.meituan.com/2021/08/26/data-warehouse-in-meituan-waimai.html)

## 编程语言

### Scala

### Python

## 理论基础

### 基本概念

#### 什么是“大数据”什么是“小数据”

### 常见算法

#### 如何实现一个布隆过滤器

### 常见计算架构

#### Lambda

![img](images/99dd755cb5a447dc1a547d782a997db1751042.jpg)

Lambda是比较经典的一款架构，以前实时的场景不是很多，以离线为主，当附加了实时场景后，由于离线和实时的时效性不同，导致技术生态是不一样的。而Lambda架构相当于附加了一条实时生产链路，在应用层面进行一个整合，双路生产，各自独立。在业务应用中，顺理成章成为了一种被采用的方式。

双路生产会存在一些问题，比如加工逻辑Double，开发运维也会Double，资源同样会变成两个资源链路。因为存在以上问题，所以又演进了一个Kappa架构。

#### Kappa

![img](images/b56fa6bad760f1dcd3a889df4c570508795045.jpg)

Kappa从架构设计来讲，比较简单，生产统一，一套逻辑同时生产离线和实时。但是在实际应用场景有比较大的局限性，在业内直接用Kappa架构生产落地的案例不多见，且场景比较单一。这些问题在美团外卖这边同样会遇到，我们也会有自己的一些思考，将会在后面的章节进行阐述。

### 编程模型

#### MapReduce

### 存储架构

#### OLAP数据库为什么要列式存储

#### 说一说HDFS

### 运筹学

## 人工智能

### 未归类

#### 贝叶斯公式是什么

#### 对特征的细粒度划分是怎么做的，有什么依据吗；

#### LSTM 讲一下，为什么比 RNN 效果更好

#### 对特征的细粒度划分是怎么做的，有什么依据吗

#### 特征对于结果的贡献怎么衡量

#### LR 和 SVM 对比

#### 数据预处理流程，建模过程

#### RF 随机森林随机性体现在哪里

#### 假设检验原理是啥，怎么做假设

#### XGBoost、GBDT、RF异同点

### OpenCV

### Web侧AI

### 推荐引擎

## 分布式文件系统

### HDFS

#### HDFS技术架构

**HDFS** （**Hadoop Distributed File System**）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：

- **客户端**：
- **NameNode（主节点）** : 负责执行有关 `文件系统命名空间` 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。
- **DataNode（数据节点）**：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。

![img](images/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f686466736172636869746563747572652e706e67.jpg)

#### HDFS写数据原理

![img](images/SouthEast.jpeg)

![img](images/SouthEast-20211011223555301.jpeg)

![img](images/SouthEast-20211011223608265.jpeg)

#### NameNode（NN）高可用实现原理

- Zookeeper 分布式协调

  > HDFS中NameNode等的HA是基于ZooKeeper实现的。它应用了ZooKeeper集群的如下功能或特性：
  >
  > 1. 只要半数以上节点还存活，就继续能对外提供服务；
  > 2. ZooKeeper通过Paxos算法提供了leader选举功能，其它follower learn leader；
  > 3. ZooKeeper提供了watcher机制，只要ZooKeeper上znode增减，或内容发生变化，或其子znode有增减，客户端都可以通过注册的watcher获得通知；
  > 4. ZooKeeper提供了持久化节点和临时节点，尤其是临时节点EPHEMERAL，其在ZooKeeper客户端连接断掉后，会自动删除。
  >
  > 正是基于ZooKeeper的上述特性，HDFS的NameNode实现了HA，NameNode的状态大致可分为Active和Standby两种，NameNode竞争在ZooKeeper指定路径上注册临时节点，将自己的host、port、nameserviceId、namenodeId等数据写入节点，哪个NameNode争先写入成功，哪个就成为Active NameNode。然后，会有后台工作线程周期性检查NameNode状态，并在一定条件下（比如Active NameNode节点发生故障等）发生竞选，由NameNode再去竞争，实现故障转移和状态切换。
  >
  > ![](images/file_1570193787000_20191004205629543913.jpg)
  >

- 隔离（Fencing）- 预防脑裂

  > 预防脑裂的常见方案就是 Fencing（隔离），思路是把旧的 Active NameNode 隔离起来，使它不能对外提供服务，保证集群在任何时候都只有一个 Active NameNode。
  >
  > HDFS 提供了 3 个级别的隔离（Fencing）：
  >
  > 1）共享存储隔离：同一时间只允许一个 NameNode 向 JournalNode 写入 EditLog 数据。
  >
  > 2）客户端隔离：同一时间只允许一个 NameNode 响应客户端的请求。
  >
  > 3）DataNode 隔离：同一时间只允许一个 NameNode 向 DataNode 下发命名空间相关的命令，例如删除、复制数据块等。

- Qurom Journal Manager 共享存储

  > 在 HDFS 的 HA 架构中还有一个非常重要的部分：Active NameNode 和 Standby NameNode 之间如何共享 EditLog 文件。
  >
  > 思路是：**Active NameNode 将日志文件写到共享存储上，Standby NameNode 实时地从共享存储读取 EditLog 文件，然后合并到 Standby NameNode 的命名空间中。一旦 Active NameNode 发生错误，Standby NameNode 就可以立即切换到 Active 状态。**
  >
  > Hadoop 2.0.3 版本开始，社区接收了由 Cloudera 公司提供基于 Qurom Journal Manager（QJM）共享存储的高可用方案，来解决 HA 架构中元数据的共享存储问题：
  >
  > http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html
  >
  > > QJM 基于 Paxos 算法实现，基本原理是：HDFS 集群中有 2n+1 台 JournalNode，EditLog 保存在 JN 的本地磁盘上；
  > >
  > > 每个 JournalNode 都允许 NmaeNode 通过它的 RPC 接口读写 EditLog 文件；
  > >
  > > 当 NmaeNode 需要写 EditLog 文件时，它会通过 QJM 向集群中所有的 JournalNode 并行发送写 EditLog 文件的请求；
  > >
  > > 每次写数据操作只要有超过一半（>=n+1）的 JournalNode 返回成功，就认为这次写操作成功了。
  >
  > 由此我们可以知道，这个 QJM 必须也是高可用的，否则 HDFS 的高可用就无法保障。
  >
  > QJM 实现 HA 的主要好处：
  >
  > > - 不存在单点故障问题；
  > > - 不需要配置额外的共享存储，降低了复杂度和维护成本；
  > > - 不需要单独配置 Fencing 实现（见文末#5.1节），因为 QJM 本身就内置了 Fencing 的功能；
  > > - 系统的鲁棒性程度是可配置的（ QJM 基于 Paxos 算法，配置 2n+1 台 JournalNode，最多能容忍 n 台机器同时挂掉）；
  > > - QJM 中存储日志的 JournalNode 不会因为其中一台的延迟而影响整体的延迟，而且也不会因为 JournalNode 的数量增多而影响性能（因为 NameNode 向 JournalNode 发送日志是并行的）。
  >
  > 关于 QJM 的具体工作原理，后面有机会了专门讲讲。

#### HDFS如何实现容错

文件系统的容错可以通过 NameNode 高可用、SecondaryNameNode 机制、数据块副本机制和心跳机制来实现。

注意：当以本地模式或者伪集群模式部署 Hadoop 时，会存在 SeconddayNameNode；当以集群模式部署 Hadoop 时，如果配置了 NameNode 的 HA 机制，则不会存在 SecondaryNameNode，此时会存在备 NameNode。

在这里重点说下集群模式下 HDFS 的容错，有关 SecondaryNameNode 机制可参见上一篇文章《前方高能 | HDFS 的架构，你吃透了吗？》的说明：

HDFS 的容错机制如图所示：

![HDFS是如何实现文件管理和容错的](images/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=.png)

具体的流程如下：

> 1. 备 NameNode 实时备份 主 NameNode 上的元数据信息，一旦主 NameNode 发生故障不可用，则备 NameNode 迅速接管主 NameNode 的工作。
>
> 2. 客户端向 NameNode 读取元数据信息。
>
> 3. NameNode 向客户端返回元数据信息。
>
> 4. 客户端向 DataNode 读取/写入 数据，此时会分为读取数据和写入数据两种情况。
>
>    > 读取数据：HDFS 会检测文件块的完整性，确认文件块的检验和是否一致，如果不一致，则从其他的 DataNode 上获取相应的副本。
>    >
>    > 写入数据：HDFS 会检测文件块的完整性，同时记录新创建的文件的所有文件块的校验和。
>
> 5. DataNode 会定期向 NameNode 发送心跳信息，将自身节点的状态告知 NameNode；NameNode 会将 DataNode 需要执行的命令放入心跳信息的返回结果中，返回给 DataNode 执行。
>
>    > 当 DataNode 发生故障没有正常发送心跳信息时，NameNode 会检测文件块的副本数是否小于 系统设置值，如果小于设置值，则自动复制新的副本并分发到其他的 DataNode 上。
>
> 6. 集群中有数据关联的 DataNode 之间复制数据副本。

当集群中的 DataNode 发生故障而失效，或者在集群中添加新的 DataNode 时，可能会导致数据分布不均匀。当某个 DataNode 上的空闲空间资源大于系统设置的临界值时，HDFS 就会从 其他的 DataNode 上将数据迁移过来。相对地，如果某个 DataNode 上的资源出现超负荷运载，HDFS 就会根据一定的规则寻找有空闲资源的 DataNode，将数据迁移过去。

还有一种从侧面说明 HDFS 支持容错的机制，即当从 HDFS 中删除数据时，数据并不是马上就会从 HDFS 中被删除，而是会将这些数据放到“回收站”目录中，随时可以恢复，直到超过了一定的时间才会真正删除这些数据。

#### Hadoop Federation（联邦）

Federation即为“联邦”，该特性允许一个HDFS集群中存在多个NameNode同时对外提供服务，这些NameNode分管一部分目录（水平切分），彼此之间相互隔离，但共享底层的DataNode存储资源。


### GFS

### Ceph

### TFS

### GridFS

## 数据仓库&数据湖

### 数据湖

#### 什么是数据湖

数据湖是在系统或者存储库中以原生格式存储数据的方法，通常使用对象块或者文件来存储各种模式和结构化的数据。目前对数据湖没有一个标准的定义，主要思想是对企业中所有数据进行统一存储，从原始数据到用于可视化、分析和机器学习等各种任务的转换数据，这些数据包括关系数据库中的结构化数据、半结构化数据（CSV、XML、JSON等）、非结构化数据（电子邮件、文件）和二进制数据（图像、音频、视频等），从而形成一个集中化数据存储系统来容纳所有形式的数据。DataLake的参考架构

![img](images/70-20210930170236724.jpg)

数据湖下面的几个特征：

- 集中的数据共享存储系统，代表性的是使用分布式文件系统（DFS）,Hadoop数据湖保存原生数据，通过数据生命周期管理来感知数据的变化，这个方法对内部的使用规则和内部审计很有用。与传统仓库相比，当需要时再讲数据进行转换、聚合和更新操作，数据管理机构对数据进行监管。

- 具有任务调度和协作能力，例如Hadoop YARN，对计算资源具有管理能力，提供Hadoop集群的持续性任务提交、安全和数据监管工具，保证用户能够获取其需要的数据和计算资源来保证分析流程的正常执行。

- 提供基于数据的一系列的应用和工作流程，由于数据以原生方式保存在数据湖中，因此要提供应用保证用户能够方便的使用数据。数据拥有者能够与数据消费者、提供者和数据操作者之间相互协调，解决共享数据遇到的技术和规则问题。

#### 数据湖和数据仓库的区别

![img](images/70.jpg)

#### Apache Calcite SQL解析为物理执行

#### 数据湖的实现有哪些技术方案

![image-20211001094615817](images/image-20211001094615817.jpg)

Apache Hudi (pronounced Hoodie) stands for `Hadoop Upserts Deletes and Incrementals`. Hudi manages the storage of large analytical datasets on DFS (Cloud stores, HDFS or any Hadoop FileSystem compatible storage).

### Druid

#### Druid有什么特点

Druid 支持将多种外部数据系统作为数据源，进行数据摄入，包括 [Hadoop](https://yuzhouwan.com/tags/Apache-Hadoop/)、[Spark](https://yuzhouwan.com/posts/4735/)、[Storm](https://yuzhouwan.com/tags/Apache-Storm/) 和 [Kafka](https://yuzhouwan.com/posts/26002/) 等

### ClickHouse

### HIVE

### sqoop

### HBase

#### HBase有什么特点

Hbase是一种NoSQL数据库，这意味着它不像传统的RDBMS数据库那样支持SQL作为查询语言。Hbase是一种分布式存储的数据库，技术上来讲，它更像是分布式存储而不是分布式数据库，它缺少很多RDBMS系统的特性，比如列类型，辅助索引，触发器，和高级查询语言等待。那Hbase有什么特性呢？如下：

- 强读写一致，但是不是“最终一致性”的数据存储，这使得它非常适合高速的计算聚合
- 自动分片，通过Region分散在集群中，当行数增长的时候，Region也会自动的切分和再分配
- 自动的故障转移
- Hadoop/HDFS集成，和HDFS开箱即用，不用太麻烦的衔接
- 丰富的“简洁，高效”API，Thrift/REST API，Java API
- 块缓存，布隆过滤器，可以高效的列查询优化
- 操作管理，Hbase提供了内置的web界面来操作，还可以监控JMX指标

#### HBase架构体系

![img](images/webp-20211011174646263)

- Zookeeper，作为分布式的协调。RegionServer也会把自己的信息写到ZooKeeper中。
- HDFS是Hbase运行的底层文件系统
- RegionServer，理解为数据节点，存储数据的。
- Master RegionServer要实时的向Master报告信息。Master知道全局的RegionServer运行情况，可以控制RegionServer的故障转移和Region的切分。

### F1 Spanner

### TiDB

## ETL工具

### Flume

### Canal

#### 说说Canal的工作原理

`Canal`是阿里开源的一款基于Mysql数据库binlog的增量订阅和消费组件，通过它可以订阅数据库的binlog日志，然后进行一些数据消费，如数据镜像、数据异构、数据索引、缓存更新等。相对于消息队列，通过这种机制可以实现数据的有序化和一致性。

![img](images/1577453-20191109095238442-801424608.jpg)

- canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议
- MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal )
- canal 解析 binary log 对象(原始为 byte 流)

#### Canal有哪些应用场景

1. 同步缓存redis/全文搜索ES

   > canal一个常见应用场景是同步缓存/全文搜索，当数据库变更后通过binlog进行缓存/ES的增量更新。当缓存/ES更新出现问题时，应该回退binlog到过去某个位置进行重新同步，并提供全量刷新缓存/ES的方法，如下图所示。
   >
   > <img src="images/1577453-20191109100411095-118737851.jpg" alt="img" style="zoom:67%;" />

2. 下发任务

   > 另一种常见应用场景是下发任务，当数据变更时需要通知其他依赖系统。其原理是任务系统监听数据库变更，然后将变更的数据写入MQ/kafka进行任务下发，比如商品数据变更后需要通知商品详情页、列表页、搜索页等先关系统。这种方式可以保证数据下发的精确性，通过MQ发送消息通知变更缓存是无法做到这一点的，而且业务系统中不会散落着各种下发MQ的代码，从而实现了下发归集，如下图所示。
   >
   > ![img](images/1577453-20191109100602058-105223593.jpg)

3. 数据异构

   > 在大型网站架构中，DB都会采用分库分表来解决容量和性能问题，但分库分表之后带来的新问题。比如不同维度的查询或者聚合查询，此时就会非常棘手。一般我们会通过数据异构机制来解决此问题。所谓的数据异构，那就是将需要join查询的多表按照某一个维度又聚合在一个DB中。让你去查询。canal就是实现数据异构的手段之一。
   >
   > <img src="images/1577453-20191109101403531-66775627.jpg" alt="img" style="zoom:80%;" />

#### Canal的HA机制

HA机制依赖基于zookeeper实现，用到的特性有watcher和EPHEMERAL节点(和session生命周期绑定)，与HDFS的HA类似。canal server和canal client分别有对应的ha实现：

- canal server: 为了减少对mysql dump的请求，*不同*server上的instance(*不同server上的相同instance*)要求同一时间只能有一个处于running，其他的处于standby状态(standby是instance的状态)。
- *canal client*: 为了保证有序性，一份instance同一时间只能由一个canal client进行get/ack/rollback操作，否则客户端接收无法保证有序。

server ha的架构图如下：

![img](images/1577453-20191109165620364-760407401.jpg)

大致步骤：

1. canal server要启动某个*canal instance*时都先向zookeeper_进行一次尝试启动判断_(实现：创建EPHEMERAL节点，谁创建成功就允许谁启动)
2. 创建zookeeper节点成功后，对应的canal server就启动对应的canal instance，*没有创建成功的canal instance就会处于standby状态*。
3. 一旦zookeeper发现canal server A创建的*instance节点*消失后，立即通知其他的canal server再次进行步骤1的操作，重新选出一个canal server启动instance。
4. canal client每次进行connect时，会首先向zookeeper询问当前是谁启动了canal instance，然后和其建立链接，一旦链接不可用，会重新尝试connect。

Canal Client的方式和canal server方式类似，也是利用zookeeper的抢占EPHEMERAL节点的方式进行控制.

### Kettle

### FileBeat

### Logstash

## 实时计算

### 综述

#### Storm、Flink、Spark实时计算方案比较

**实时计算方案列表如下：**

| 项目/引擎 | Storm                                       | Flink                                                        | spark-streaming                                              |
| :-------- | :------------------------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| API       | 灵活的底层 API 和具有事务保证的 Trident API | 流 API 和更加适合数据开发的 Table API 和 Flink SQL 支持      | 流 API 和 Structured-Streaming API 同时也可以使用更适合数据开发的 Spark SQL |
| 容错机制  | ACK 机制                                    | State 分布式快照保存点                                       | RDD 保存点                                                   |
| 状态管理  | Trident State状态管理                       | Key State 和 Operator State两种 State 可以使用，支持多种持久化方案 | 有 UpdateStateByKey 等 API 进行带状态的变更，支持多种持久化方案 |
| 处理模式  | 单条流式处理                                | 单条流式处理                                                 | Mic batch处理                                                |
| 延迟      | 毫秒级                                      | 毫秒级                                                       | 秒级                                                         |
| 语义保障  | At Least Once，Exactly Once                 | Exactly Once，At Least Once                                  | At Least Once                                                |

从调研结果来看，Flink 和 Spark Streaming 的 API 、容错机制与状态持久化机制都可以解决一部分我们目前使用 Storm 中遇到的问题。但 Flink 在数据延迟上和 Storm 更接近，对现有应用影响最小。而且在公司内部的测试中 Flink 的吞吐性能对比 Storm 有十倍左右提升。综合考量可以选定 Flink 引擎作为实时数仓的开发引擎。

### Flink

#### 说说Flink的状态后端

- State backend 的选择

  > | StateBackend        | in-flight     | checkpoint | 吞吐 | 推荐使用场景                             |
  > | :------------------ | :------------ | :--------- | :--- | :--------------------------------------- |
  > | MemoryStateBackend  | TM Memory     | JM Memory  | 高   | 调试、小状态或对数据丢失或重复无要求     |
  > | FsStateBackend      | TM Memory     | FS/HDFS    | 高   | 普通状态、窗口、KV 结构、高可用          |
  > | RocksDBStateBackend | RocksDB on TM | FS/HDFS    | 低   | 超大状态、超长窗口、大型 KV 结构、高可用 |

#### Flink如何管理内存

- 堆内存管理（积极的内存管理）

  > Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上，这个内存块叫做 `MemorySegment`，它代表了一段固定长度的内存（默认大小为 32KB），也是 Flink 中最小的内存分配单元，并且提供了非常高效的读写方法。你可以把 MemorySegment 想象成是为 Flink 定制的 `java.nio.ByteBuffer`。它的底层可以是一个普通的 Java 字节数组（`byte[]`），也可以是一个申请在堆外的 `ByteBuffer`。每条记录都会以序列化的形式存储在一个或多个`MemorySegment`中。Flink 中的 Worker 名叫 TaskManager，是用来运行用户代码的 JVM 进程。TaskManager 的堆内存主要被分成了三个部分：
  >
  > [<img src="images/TB17qs5JpXXXXXhXpXXXXXXXXXX.jpg" alt="img" style="zoom:50%;" />](http://img3.tbcdn.cn/5476e8b07b923/TB17qs5JpXXXXXhXpXXXXXXXXXX)
  >
  > - **Network Buffers:** 一定数量的32KB大小的 buffer，主要用于数据的网络传输。在 TaskManager 启动的时候就会分配。默认数量是 2048 个，可以通过 `taskmanager.network.numberOfBuffers` 来配置。（阅读[这篇文章](http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/#网络传输中的内存管理)了解更多Network Buffer的管理）
  > - **Memory Manager Pool:** 这是一个由 `MemoryManager` 管理的，由众多`MemorySegment`组成的超大集合。Flink 中的算法（如 sort/shuffle/join）会向这个内存池申请 MemorySegment，将序列化后的数据存于其中，使用完后释放回内存池。默认情况下，池子占了堆内存的 70% 的大小。
  > - **Remaining (Free) Heap:** 这部分的内存是留给用户代码以及 TaskManager 的数据结构使用的。因为这些数据结构一般都很小，所以基本上这些内存都是给用户代码使用的。从GC的角度来看，可以把这里看成的新生代，也就是说这里主要都是由用户代码生成的短期对象。
  >
  > **注意：Memory Manager Pool 主要在Batch模式下使用。在Steaming模式下，该池子不会预分配内存，也不会向该池子请求内存块。也就是说该部分的内存都是可以给用户代码使用的。不过社区是打算在 Streaming 模式下也能将该池子利用起来。**
  >
  > Flink 采用类似 DBMS 的 sort 和 join 算法，直接操作二进制数据，从而使序列化/反序列化带来的开销达到最小。所以 Flink 的内部实现更像 C/C++ 而非 Java。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。如果要操作多块MemorySegment就像操作一块大的连续内存一样，Flink会使用逻辑视图（`AbstractPagedInputView`）来方便操作。下图描述了 Flink 如何存储序列化后的数据到内存块中，以及在需要的时候如何将数据存储到磁盘上。
  >
  > 从上面我们能够得出 Flink 积极的内存管理以及直接操作二进制数据有以下几点好处：
  >
  > 1. **减少GC压力。**显而易见，因为所有常驻型数据都以二进制的形式存在 Flink 的`MemoryManager`中，这些`MemorySegment`一直呆在老年代而不会被GC回收。其他的数据对象基本上是由用户代码生成的短生命周期对象，这部分对象可以被 Minor GC 快速回收。只要用户不去创建大量类似缓存的常驻型对象，那么老年代的大小是不会变的，Major GC也就永远不会发生。从而有效地降低了垃圾回收的压力。另外，这里的内存块还可以是堆外内存，这可以使得 JVM 内存更小，从而加速垃圾回收。
  > 2. **避免了OOM。**所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。在内存吃紧的情况下，算法（sort/join等）会高效地将一大批内存块写到磁盘，之后再读回来。因此，`OutOfMemoryErrors`可以有效地被避免。
  > 3. **节省内存空间。**Java 对象在存储上有很多额外的消耗（如上一节所谈）。如果只存储实际数据的二进制内容，就可以避免这部分消耗。
  > 4. **高效的二进制操作 & 缓存友好的计算。**二进制数据以定义好的格式存储，可以高效地比较与操作。另外，该二进制形式可以把相关的值，以及hash值，键值和指针等相邻地放进内存中。这使得数据结构可以对高速缓存更友好，可以从 L1/L2/L3 缓存获得性能的提升（下文会详细解释）。

- Flink 基于堆内存的内存管理机制已经可以解决很多JVM现存问题了，为什么还要引入堆外内存？

  > 1. 启动超大内存（上百GB）的JVM需要很长时间，GC停留时间也会很长（分钟级）。使用堆外内存的话，可以极大地减小堆内存（只需要分配Remaining Heap那一块），使得 TaskManager 扩展到上百GB内存不是问题。
  > 2. 高效的 IO 操作。堆外内存在写磁盘或网络传输时是 zero-copy，而堆内存的话，至少需要 copy 一次。
  > 3. 堆外内存是进程间共享的。也就是说，即使JVM进程崩溃也不会丢失数据。这可以用来做故障恢复（Flink暂时没有利用起这个，不过未来很可能会去做）。
  >
  > 但是强大的东西总是会有其负面的一面，不然为何大家不都用堆外内存呢。
  >
  > 1. 堆内存的使用、监控、调试都要简单很多。堆外内存意味着更复杂更麻烦。
  > 2. Flink 有时需要分配短生命周期的 `MemorySegment`，这个申请在堆上会更廉价。
  > 3. 有些操作在堆内存上会快一点点。
  >
  > Flink用通过`ByteBuffer.allocateDirect(numBytes)`来申请堆外内存，用 `sun.misc.Unsafe` 来操作堆外内存。

### Spark Streaming

### Kafka Streams

### Storm

## 离线计算

### Spark

### MapReduce

## 其它

### 场景题

#### 1亿个正整数,范围是0-42亿。求出现次数是2的数字，空间复杂度

#### 有一个IP地址库，假设有几十万条ip，如何判断某个ip地址是否在这个库中？

思路1：Bitmap：将每一条ip对应位图中的一个位，2^32次方(42亿多)个数据只需要512M空间。可以实现O(1)的查询复杂度

思路2：布隆过滤器

#### 在一个文件中有 10G 个整数,乱序排列,要求找出中位数(内存限制为 2G)

平衡二叉树，二叉堆？内存不足时两侧同时丢弃相同数量元素，平衡后的roott节点为中位数

#### 一个5T的文件，里面全是id，1-10^9 ，如何计算不同id的个数？

#### 海量日志数据，提取出某日访问百度次数最多的那个IP

#### 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词

#### 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数

#### 怎么在海量数据中找出重复次数最多的一个？

#### 100w个数中找出最大的100个数

1. 最小堆，找最大100个数
2. 快速排序
3. 选取前100个元素，排序，然后扫描剩余的元素，与排好序的元素中最小的相比，如果比它大，替换，重排前面，这跟堆排序思路一样

#### 40亿个不重复无序的unsigned int的整数，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

思路1：位图，40亿个不重复的数，每个数用1bit表示，出现或不出现，40\*10^8*1 = 0.5G大小。遍历这40亿个数，如果出现将对应位置为1，对于给定的数直接判断位图中对应的值。

思路2：将每个整数都看成32位的二进制数，从最高位，依次按位来分，按最高位0，1分成两个文件，每个文件数字个数小于20亿，与所要判断的数的最高为进行比较，从而知道去哪个文件继续比较，然后对于选定的文件再按照次高位比较再分成2个文件，再比较判断数对应的位数，依次循环，直到最后一位，就可以找到或判断没有该数了。时间复杂度O（log2n）,因为每次都将数据减少一半，直到最后一个。

#### 5亿个整数找他们的中位数

中位数的定义：一个给定排序好的序列，奇数个的话，我们就取中间的一个；偶数个的话，我们一般取中间两个数的平均值；因此对于本题，我们需得到中间的第50亿和第50亿+1这两个数；

#### 给定a、b两个文件各存50亿个url（每个占64字节）内存限制是4G，让你找出a、b文件共同的url

思路：每个文件的大小5G*64 = 32G，远远大于内存，需要对a，b分别分成小文件

利用一个hash(url)00，分别将a，b文件分别映射成1000个小文件，因为通过相同的映射函数，所以对于a，b，相同的url都在对应的文件中，（a0 vs b0, a1 vs b1等等）分别比对这1000个对应的小文件，可以通过先将a映射到一个hash表中，然后依次遍历b，判断是否在a中出现，出现过则说明重复。



