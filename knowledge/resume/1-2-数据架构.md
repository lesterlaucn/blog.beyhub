资料

https://blog.csdn.net/qq_26803795/category_9271568.html

[美团外卖实时数仓建设实践](https://tech.meituan.com/2021/08/26/data-warehouse-in-meituan-waimai.html)

## 编程语言

### Scala

### Python

## 理论基础

### 基本概念

#### 什么是“大数据”什么是“小数据”

### 常见算法

#### 如何实现一个布隆过滤器

#### LSM-Tree

### 常见计算架构

#### Lambda

![img](images/99dd755cb5a447dc1a547d782a997db1751042.jpg)

Lambda是比较经典的一款架构，以前实时的场景不是很多，以离线为主，当附加了实时场景后，由于离线和实时的时效性不同，导致技术生态是不一样的。而Lambda架构相当于附加了一条实时生产链路，在应用层面进行一个整合，双路生产，各自独立。在业务应用中，顺理成章成为了一种被采用的方式。

双路生产会存在一些问题，比如加工逻辑Double，开发运维也会Double，资源同样会变成两个资源链路。因为存在以上问题，所以又演进了一个Kappa架构。

#### Kappa

![img](images/b56fa6bad760f1dcd3a889df4c570508795045.jpg)

Kappa从架构设计来讲，比较简单，生产统一，一套逻辑同时生产离线和实时。但是在实际应用场景有比较大的局限性，在业内直接用Kappa架构生产落地的案例不多见，且场景比较单一。这些问题在美团外卖这边同样会遇到，我们也会有自己的一些思考，将会在后面的章节进行阐述。

### 编程模型

#### MapReduce

### 存储架构

#### OLAP数据库为什么要列式存储

#### 说一说HDFS

### 运筹学

## 人工智能

### 未归类

#### 贝叶斯公式是什么

#### 对特征的细粒度划分是怎么做的，有什么依据吗；

#### LSTM 讲一下，为什么比 RNN 效果更好

#### 对特征的细粒度划分是怎么做的，有什么依据吗

#### 特征对于结果的贡献怎么衡量

#### LR 和 SVM 对比

#### 数据预处理流程，建模过程

#### RF 随机森林随机性体现在哪里

#### 假设检验原理是啥，怎么做假设

#### XGBoost、GBDT、RF异同点

### OpenCV

### Web侧AI

### 推荐引擎

### 十大经典算法

#### KNN近邻

#### 线性回归

#### 逻辑回归

#### 凸优化

#### 朴素贝叶斯

#### 支持向量机

#### 决策树

#### 随机森林

#### GBDT

#### XGBoost

#### 神经网络

#### K-Means

#### 层次聚类

#### 主题模型

#### Word2Vec

#### PCA

## 分布式存储系统

### HDFS

#### HDFS技术架构

**HDFS** （**Hadoop Distributed File System**）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：

- **客户端**：
- **NameNode（主节点）** : 负责执行有关 `文件系统命名空间` 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。
- **DataNode（数据节点）**：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。

![img](images/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f686466736172636869746563747572652e706e67.jpg)

#### HDFS写数据原理

![img](images/SouthEast.jpeg)

![img](images/SouthEast-20211011223555301.jpeg)

![img](images/SouthEast-20211011223608265.jpeg)

#### NameNode（NN）高可用实现原理

- Zookeeper 分布式协调

  > HDFS中NameNode等的HA是基于ZooKeeper实现的。它应用了ZooKeeper集群的如下功能或特性：
  >
  > 1. 只要半数以上节点还存活，就继续能对外提供服务；
  > 2. ZooKeeper通过Paxos算法提供了leader选举功能，其它follower learn leader；
  > 3. ZooKeeper提供了watcher机制，只要ZooKeeper上znode增减，或内容发生变化，或其子znode有增减，客户端都可以通过注册的watcher获得通知；
  > 4. ZooKeeper提供了持久化节点和临时节点，尤其是临时节点EPHEMERAL，其在ZooKeeper客户端连接断掉后，会自动删除。
  >
  > 正是基于ZooKeeper的上述特性，HDFS的NameNode实现了HA，NameNode的状态大致可分为Active和Standby两种，NameNode竞争在ZooKeeper指定路径上注册临时节点，将自己的host、port、nameserviceId、namenodeId等数据写入节点，哪个NameNode争先写入成功，哪个就成为Active NameNode。然后，会有后台工作线程周期性检查NameNode状态，并在一定条件下（比如Active NameNode节点发生故障等）发生竞选，由NameNode再去竞争，实现故障转移和状态切换。
  >
  > ![](images/file_1570193787000_20191004205629543913.jpg)
  >

- 隔离（Fencing）- 预防脑裂

  > 预防脑裂的常见方案就是 Fencing（隔离），思路是把旧的 Active NameNode 隔离起来，使它不能对外提供服务，保证集群在任何时候都只有一个 Active NameNode。
  >
  > HDFS 提供了 3 个级别的隔离（Fencing）：
  >
  > 1）共享存储隔离：同一时间只允许一个 NameNode 向 JournalNode 写入 EditLog 数据。
  >
  > 2）客户端隔离：同一时间只允许一个 NameNode 响应客户端的请求。
  >
  > 3）DataNode 隔离：同一时间只允许一个 NameNode 向 DataNode 下发命名空间相关的命令，例如删除、复制数据块等。

- Qurom Journal Manager 共享存储

  > 在 HDFS 的 HA 架构中还有一个非常重要的部分：Active NameNode 和 Standby NameNode 之间如何共享 EditLog 文件。
  >
  > 思路是：**Active NameNode 将日志文件写到共享存储上，Standby NameNode 实时地从共享存储读取 EditLog 文件，然后合并到 Standby NameNode 的命名空间中。一旦 Active NameNode 发生错误，Standby NameNode 就可以立即切换到 Active 状态。**
  >
  > Hadoop 2.0.3 版本开始，社区接收了由 Cloudera 公司提供基于 Qurom Journal Manager（QJM）共享存储的高可用方案，来解决 HA 架构中元数据的共享存储问题：
  >
  > http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html
  >
  > > QJM 基于 Paxos 算法实现，基本原理是：HDFS 集群中有 2n+1 台 JournalNode，EditLog 保存在 JN 的本地磁盘上；
  > >
  > > 每个 JournalNode 都允许 NmaeNode 通过它的 RPC 接口读写 EditLog 文件；
  > >
  > > 当 NmaeNode 需要写 EditLog 文件时，它会通过 QJM 向集群中所有的 JournalNode 并行发送写 EditLog 文件的请求；
  > >
  > > 每次写数据操作只要有超过一半（>=n+1）的 JournalNode 返回成功，就认为这次写操作成功了。
  >
  > 由此我们可以知道，这个 QJM 必须也是高可用的，否则 HDFS 的高可用就无法保障。
  >
  > QJM 实现 HA 的主要好处：
  >
  > > - 不存在单点故障问题；
  > > - 不需要配置额外的共享存储，降低了复杂度和维护成本；
  > > - 不需要单独配置 Fencing 实现（见文末#5.1节），因为 QJM 本身就内置了 Fencing 的功能；
  > > - 系统的鲁棒性程度是可配置的（ QJM 基于 Paxos 算法，配置 2n+1 台 JournalNode，最多能容忍 n 台机器同时挂掉）；
  > > - QJM 中存储日志的 JournalNode 不会因为其中一台的延迟而影响整体的延迟，而且也不会因为 JournalNode 的数量增多而影响性能（因为 NameNode 向 JournalNode 发送日志是并行的）。
  >
  > 关于 QJM 的具体工作原理，后面有机会了专门讲讲。

#### HDFS如何实现容错

文件系统的容错可以通过 NameNode 高可用、SecondaryNameNode 机制、数据块副本机制和心跳机制来实现。

注意：当以本地模式或者伪集群模式部署 Hadoop 时，会存在 SeconddayNameNode；当以集群模式部署 Hadoop 时，如果配置了 NameNode 的 HA 机制，则不会存在 SecondaryNameNode，此时会存在备 NameNode。

在这里重点说下集群模式下 HDFS 的容错，有关 SecondaryNameNode 机制可参见上一篇文章《前方高能 | HDFS 的架构，你吃透了吗？》的说明：

HDFS 的容错机制如图所示：

![HDFS是如何实现文件管理和容错的](images/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=.png)

具体的流程如下：

> 1. 备 NameNode 实时备份 主 NameNode 上的元数据信息，一旦主 NameNode 发生故障不可用，则备 NameNode 迅速接管主 NameNode 的工作。
>
> 2. 客户端向 NameNode 读取元数据信息。
>
> 3. NameNode 向客户端返回元数据信息。
>
> 4. 客户端向 DataNode 读取/写入 数据，此时会分为读取数据和写入数据两种情况。
>
>    > 读取数据：HDFS 会检测文件块的完整性，确认文件块的检验和是否一致，如果不一致，则从其他的 DataNode 上获取相应的副本。
>    >
>    > 写入数据：HDFS 会检测文件块的完整性，同时记录新创建的文件的所有文件块的校验和。
>
> 5. DataNode 会定期向 NameNode 发送心跳信息，将自身节点的状态告知 NameNode；NameNode 会将 DataNode 需要执行的命令放入心跳信息的返回结果中，返回给 DataNode 执行。
>
>    > 当 DataNode 发生故障没有正常发送心跳信息时，NameNode 会检测文件块的副本数是否小于 系统设置值，如果小于设置值，则自动复制新的副本并分发到其他的 DataNode 上。
>
> 6. 集群中有数据关联的 DataNode 之间复制数据副本。

当集群中的 DataNode 发生故障而失效，或者在集群中添加新的 DataNode 时，可能会导致数据分布不均匀。当某个 DataNode 上的空闲空间资源大于系统设置的临界值时，HDFS 就会从 其他的 DataNode 上将数据迁移过来。相对地，如果某个 DataNode 上的资源出现超负荷运载，HDFS 就会根据一定的规则寻找有空闲资源的 DataNode，将数据迁移过去。

还有一种从侧面说明 HDFS 支持容错的机制，即当从 HDFS 中删除数据时，数据并不是马上就会从 HDFS 中被删除，而是会将这些数据放到“回收站”目录中，随时可以恢复，直到超过了一定的时间才会真正删除这些数据。
#### Hadoop Federation（联邦）

Federation即为“联邦”，该特性允许一个HDFS集群中存在多个NameNode同时对外提供服务，这些NameNode分管一部分目录（水平切分），彼此之间相互隔离，但共享底层的DataNode存储资源。

### GFS

### Ceph

### TFS

### GridFS

### Bookkeeper

#### Bookkeeper适合哪些场景

- WAL（write-ahead logging）：bookkeeper可作为wal方案

  > 消息中间件最最基础也是最核心的部分：write-ahead logging(WAL)。
  >
  > 在计算机科学中，**「预写式日志」**（Write-ahead logging，缩写 WAL）是关系数据库系统中用于提供原子性和持久性（ACID 属性中的两个）的一系列技术。在使用 WAL 的系统中，所有的修改在提交之前都要先写入 log 文件中。
  >
  > log 文件中通常包括 redo 和 undo 信息。这样做的目的可以通过一个例子来说明。假设一个程序在执行某些操作的过程中机器掉电了。在重新启动时，程序可能需要知道当时执行的操作是成功了还是部分成功或者是失败了。如果使用了 WAL，程序就可以检查 log 文件，并对突然掉电时计划执行的操作内容跟实际上执行的操作内容进行比较。在这个比较的基础上，程序就可以决定是撤销已做的操作还是继续完成已做的操作，或者是保持原样。
  >
  > WAL 允许用 in-place 方式更新数据库。另一种用来实现原子更新的方法是 shadow paging，它并不是 in-place 方式。用 in-place 方式做更新的主要优点是减少索引和块列表的修改。ARIES 是 WAL 系列技术常用的算法。在文件系统中，WAL 通常称为 journaling。PostgreSQL 也是用 WAL 来提供 point-in-time 恢复和数据库复制特性。
  >
  > **「修改并不直接写入到数据库文件中，而是写入到另外一个称为 WAL 的文件中；如果事务失败，WAL 中的记录会被忽略，撤销修改；如果事务成功，它将在随后的某个时间被写回到数据库文件中，提交修改。」**
  >
  > 1. 读和写可以完全地并发执行，不会互相阻塞（但是写之间仍然不能并发）。
  > 2. WAL 在大多数情况下，拥有更好的性能（因为无需每次写入时都要写两个文件）。
  > 3. 磁盘 I/O 行为更容易被预测。
  > 4. 使用更少的 fsync()操作，减少系统脆弱的问题。

- 流存储：比如pulsar通过bookkeeper存储消息

- 对象/Blob存储

#### WAL在消息中间件中的应用

WAL可以说是消息中间件的基础，也是所有存储类系统的基础。在消息中间件中，WAL没有MySQL中那么复杂，我们只需要记redo log。消息直接存储在redo log中，只要写redo log完成了，那么消息就写入完成了：

1. 消息写入redo log就表明持久化了
2. 且不会出现原子性的问题，消息写入即成功，没写入即失败

- 存储结构

使用WAL存储数据，就需要去组织存储文件，比如MySQL的binlog文件。在消息中间件中也需要类似的形式去组织redo log，即消息的存储文件。

我们采用固定大小的存储文件，这样在索引消息的时候，只要知道偏移量，就能找到对应的存储文件。比如下面是文件大小为1024的存储文件示例：![img](images/0-20211022095320239.png)

消息被不断的追加到最新的存储文件中。

![img](images/0.png)

消息在文件中的存储格式大致如上：

- 采用二进制的格式存储消息
- 消息是不定长的

所以这里会需要一个结构来索引消息。索引到一条完整的消息只需要两个元素：偏移量、大小。只要有这两个元素就可以从存储日志中读取一段完整的数据（一条完整的消息）。

![img](images/0-20211022095320196.png)

以上的结构是最简单的消息中间件存储的模型，虽然离真正应用到实践中还有一些距离，但是核心思想是一致的。

考虑这几个问题：

1. 消息具体的存储协议，即存储文件中消息需要包含哪些内容
2. 如何优化索引结构，支持消息回溯、消息过滤等功能

## 数据仓库&数据湖

### 数据湖

#### 什么是数据湖

数据湖是在系统或者存储库中以原生格式存储数据的方法，通常使用对象块或者文件来存储各种模式和结构化的数据。目前对数据湖没有一个标准的定义，主要思想是对企业中所有数据进行统一存储，从原始数据到用于可视化、分析和机器学习等各种任务的转换数据，这些数据包括关系数据库中的结构化数据、半结构化数据（CSV、XML、JSON等）、非结构化数据（电子邮件、文件）和二进制数据（图像、音频、视频等），从而形成一个集中化数据存储系统来容纳所有形式的数据。DataLake的参考架构。

![img](images/70-20210930170236724.jpg)

数据湖下面的几个特征：

- 集中的数据共享存储系统，代表性的是使用分布式文件系统（DFS）,Hadoop数据湖保存原生数据，通过数据生命周期管理来感知数据的变化，这个方法对内部的使用规则和内部审计很有用。与传统仓库相比，当需要时再讲数据进行转换、聚合和更新操作，数据管理机构对数据进行监管。

- 具有任务调度和协作能力，例如Hadoop YARN，对计算资源具有管理能力，提供Hadoop集群的持续性任务提交、安全和数据监管工具，保证用户能够获取其需要的数据和计算资源来保证分析流程的正常执行。

- 提供基于数据的一系列的应用和工作流程，由于数据以原生方式保存在数据湖中，因此要提供应用保证用户能够方便的使用数据。数据拥有者能够与数据消费者、提供者和数据操作者之间相互协调，解决共享数据遇到的技术和规则问题。

#### 数据湖和数据仓库的区别

![img](images/70.jpg)

#### Apache Calcite SQL解析为物理执行

#### 数据湖的实现有哪些技术方案

![image-20211001094615817](images/image-20211001094615817.jpg)

Apache Hudi (pronounced Hoodie) stands for `Hadoop Upserts Deletes and Incrementals`. Hudi manages the storage of large analytical datasets on DFS (Cloud stores, HDFS or any Hadoop FileSystem compatible storage).

### Druid

#### Druid有什么特点

Druid 支持将多种外部数据系统作为数据源，进行数据摄入，包括 [Hadoop](https://yuzhouwan.com/tags/Apache-Hadoop/)、[Spark](https://yuzhouwan.com/posts/4735/)、[Storm](https://yuzhouwan.com/tags/Apache-Storm/) 和 [Kafka](https://yuzhouwan.com/posts/26002/) 等

### ClickHouse

### HIVE

#### 什么是HIVE

**Hive的本质是将 SQL 语句转换为 MapReduce 任务运行

####  hive默认执行引擎是什么

HR(MapReduce)，

可替换为Tez：

```shell
set hive.execution.engine=mr;
set hive.execution.engine=spark;
```

### Sqoop

### HBase

#### HBase有什么特点

Hbase是一种NoSQL数据库，这意味着它不像传统的RDBMS数据库那样支持SQL作为查询语言。Hbase是一种分布式存储的数据库，技术上来讲，它更像是分布式存储而不是分布式数据库，它缺少很多RDBMS系统的特性，比如列类型，辅助索引，触发器，和高级查询语言等待。那Hbase有什么特性呢？如下：

- 强读写一致，但是不是“最终一致性”的数据存储，这使得它非常适合高速的计算聚合
- 自动分片，通过Region分散在集群中，当行数增长的时候，Region也会自动的切分和再分配
- 自动的故障转移
- Hadoop/HDFS集成，和HDFS开箱即用，不用太麻烦的衔接
- 丰富的“简洁，高效”API，Thrift/REST API，Java API
- 块缓存，布隆过滤器，可以高效的列查询优化
- 操作管理，Hbase提供了内置的web界面来操作，还可以监控JMX指标

#### HBase架构体系

![img](images/webp-20211011174646263)

- Zookeeper，作为分布式的协调。RegionServer也会把自己的信息写到ZooKeeper中。
- HDFS是Hbase运行的底层文件系统
- RegionServer，理解为数据节点，存储数据的。
- Master RegionServer要实时的向Master报告信息。Master知道全局的RegionServer运行情况，可以控制RegionServer的故障转移和Region的切分。

### F1 Spanner

### TiDB

https://docs.pingcap.com/zh/tidb/stable

#### 存储引擎TiKV：整体架构

![TiKV RocksDB](images/tikv-rocksdb.png)

RocksDB 作为 TiKV 的核心存储引擎，用于存储 Raft 日志以及用户数据。每个 TiKV 实例中有两个 RocksDB 实例，一个用于存储 Raft 日志（通常被称为 raftdb），另一个用于存储用户数据以及 MVCC 信息（通常被称为 kvdb）。kvdb 中有四个 ColumnFamily：raft、lock、default 和 write：

- raft 列：用于存储各个 Region 的元信息。仅占极少量空间，用户可以不必关注。
- lock 列：用于存储悲观事务的悲观锁以及分布式事务的一阶段 Prewrite 锁。当用户的事务提交之后，lock cf 中对应的数据会很快删除掉，因此大部分情况下 lock cf 中的数据也很少（少于 1GB）。如果 lock cf 中的数据大量增加，说明有大量事务等待提交，系统出现了 bug 或者故障。
- write 列：用于存储用户真实的写入数据以及 MVCC 信息（该数据所属事务的开始时间以及提交时间）。当用户写入了一行数据时，如果该行数据长度小于 255 字节，那么会被存储 write 列中，否则的话该行数据会被存入到 default 列中。由于 TiDB 的非 unique 索引存储的 value 为空，unique 索引存储的 value 为主键索引，因此二级索引只会占用 writecf 的空间。
- default 列：用于存储超过 255 字节长度的数据。

与传统的整节点备份方式不同，TiKV 参考 Spanner 设计了 multi-raft-group 的副本机制。将数据按照 key 的范围划分成大致相等的切片（下文统称为 Region），每一个切片会有多个副本（通常是 3 个），其中一个副本是 Leader，提供读写服务。TiKV 通过 PD 对这些 Region 以及副本进行调度，以保证数据和读写负载都均匀地分散在各个 TiKV 上，这样的设计保证了整个集群资源的充分利用并且可以随着机器数量的增加水平扩展。

![TiKV 架构](images/tikv-arch.png)

#### 存储引擎TiKV：Region与RocksDB

虽然 TiKV 将数据按照范围切割成了多个 Region，但是同一个节点的所有 Region 数据仍然是不加区分地存储于同一个 RocksDB 实例上，而用于 Raft 协议复制所需要的日志则存储于另一个 RocksDB 实例。这样设计的原因是因为随机 I/O 的性能远低于顺序 I/O，所以 TiKV 使用同一个 RocksDB 实例来存储这些数据，以便不同 Region 的写入可以合并在一次 I/O 中。

#### 存储引擎TiKV：Region与Raft

Region 与副本之间通过 Raft 协议来维持数据一致性，任何写请求都只能在 Leader 上写入，并且需要写入多数副本后（默认配置为 3 副本，即所有请求必须至少写入两个副本成功）才会返回客户端写入成功。

当某个 Region 的大小超过一定限制（默认是 144MB）后，TiKV 会将它分裂为两个或者更多个 Region，以保证各个 Region 的大小是大致接近的，这样更有利于 PD 进行调度决策。同样，当某个 Region 因为大量的删除请求导致 Region 的大小变得更小时，TiKV 会将比较小的两个相邻 Region 合并为一个。

当 PD 需要把某个 Region 的一个副本从一个 TiKV 节点调度到另一个上面时，PD 会先为这个 Raft Group 在目标节点上增加一个 Learner 副本（虽然会复制 Leader 的数据，但是不会计入写请求的多数副本中）。当这个 Learner 副本的进度大致追上 Leader 副本时，Leader 会将它变更为 Follower，之后再移除操作节点的 Follower 副本，这样就完成了 Region 副本的一次调度。

Leader 副本的调度原理也类似，不过需要在目标节点的 Learner 副本变为 Follower 副本后，再执行一次 Leader Transfer，让该 Follower 主动发起一次选举成为新 Leader，之后新 Leader 负责删除旧 Leader 这个副本。

### RocksDB

[RocksDB](https://github.com/facebook/rocksdb) 是由 Facebook 基于 LevelDB 开发的一款提供键值存储与读写功能的 LSM-tree 架构引擎。用户写入的键值对会先写入磁盘上的 WAL (Write Ahead Log)，然后再写入内存中的跳表（SkipList，这部分结构又被称作 MemTable）。LSM-tree 引擎由于将用户的随机修改（插入）转化为了对 WAL 文件的顺序写，因此具有比 B 树类存储引擎更高的写吞吐。

#### 索引机制

RocksDB采用的内存结构memtable和外存结构SST，决定了RocksDB能够支持点查和范围查询。

1. Memtable

> 内存中的数据结构，在数据flush到SST之前用来保存数据的，可用于读写。写入数据时首先插入memtable，写满后转为immutable memtable，等待flush到SST中。读取数据时也是优先读取memtable中，数据相对于SST比较新。支持如下两类数据结构：
>
> - Skiplist MemTable：基于skiplist的memtable为读写、随机访问和顺序扫描提供了良好的性能，支持范围查询
> - HashSkiplist MemTable：hash和skiplist的结合，按照key的前缀做hash，每个hash桶中都是一个skiplist。单独访问一个key时性能更好，相对于skiplist减少了比较次数，夸多个前缀进行扫描需要复制和排序，范围查询性能也会差一些。

2. SST

SST是Sorted Sequence Table（排序队列表），是排好序的数据文件。在这些文件里，所有键都按照排序好的顺序组织，一个键或者一个迭代位置可以通过二分查找进行定位，支持两种结构：

- Block-based Table格式：该方式是RocksDB的默认SST格式。内部结构按照块的方式组织，详情见[github wiki](https://github.com/facebook/rocksdb/wiki/Rocksdb-BlockBasedTable-Format)。

```
<文件开始>
[data block 1]                                (排好序的KV，二分查找)
[data block 2]
...
[data block N]
[meta block 1: filter block]                  (过滤器)
[meta block 2: stats block]                   (属性)
[meta block 3: compression dictionary block]  (压缩字典)
[meta block 4: range deletion block]          (see section: "range deletion" Meta Block)
...
[meta block K: future extended block]  (后期会添加更多的元数据块)
[metaindex block]                      (元数据索引块，指向多个元数据块位置)
[index block]                          (数据索引块)
    [index block - partition 1]        (按照Key的范围创建的索引)
    [index block - partition 2]
    ...
    [index block - partition N]
    [index block - top-level index]    (先将顶级索引加载到内存，然后按需加载对应分区索引)
[Footer]                               (固定大小; 指定数据索引块的top-level index和元数据索引块)
<文件结束>
```

- PlainTable格式：RocksDB针对纯内存或低延迟介质上的低查询延迟进行了优化。[详情见github wiki](https://github.com/facebook/rocksdb/wiki/PlainTable-Format)。

## ETL工具

### Flume

### Canal

#### 说说Canal的工作原理

`Canal`是阿里开源的一款基于Mysql数据库binlog的增量订阅和消费组件，通过它可以订阅数据库的binlog日志，然后进行一些数据消费，如数据镜像、数据异构、数据索引、缓存更新等。相对于消息队列，通过这种机制可以实现数据的有序化和一致性。

![img](images/1577453-20191109095238442-801424608.jpg)

- canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议
- MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal )
- canal 解析 binary log 对象(原始为 byte 流)

#### Canal有哪些应用场景

1. 同步缓存redis/全文搜索ES

   > canal一个常见应用场景是同步缓存/全文搜索，当数据库变更后通过binlog进行缓存/ES的增量更新。当缓存/ES更新出现问题时，应该回退binlog到过去某个位置进行重新同步，并提供全量刷新缓存/ES的方法，如下图所示。
   >
   > <img src="images/1577453-20191109100411095-118737851.jpg" alt="img" style="zoom:67%;" />

2. 下发任务

   > 另一种常见应用场景是下发任务，当数据变更时需要通知其他依赖系统。其原理是任务系统监听数据库变更，然后将变更的数据写入MQ/kafka进行任务下发，比如商品数据变更后需要通知商品详情页、列表页、搜索页等先关系统。这种方式可以保证数据下发的精确性，通过MQ发送消息通知变更缓存是无法做到这一点的，而且业务系统中不会散落着各种下发MQ的代码，从而实现了下发归集，如下图所示。
   >
   > ![img](images/1577453-20191109100602058-105223593.jpg)

3. 数据异构

   > 在大型网站架构中，DB都会采用分库分表来解决容量和性能问题，但分库分表之后带来的新问题。比如不同维度的查询或者聚合查询，此时就会非常棘手。一般我们会通过数据异构机制来解决此问题。所谓的数据异构，那就是将需要join查询的多表按照某一个维度又聚合在一个DB中。让你去查询。canal就是实现数据异构的手段之一。
   >
   > <img src="images/1577453-20191109101403531-66775627.jpg" alt="img" style="zoom:80%;" />

#### Canal的HA机制

HA机制依赖基于zookeeper实现，用到的特性有watcher和EPHEMERAL节点(和session生命周期绑定)，与HDFS的HA类似。canal server和canal client分别有对应的ha实现：

- canal server: 为了减少对mysql dump的请求，*不同*server上的instance(*不同server上的相同instance*)要求同一时间只能有一个处于running，其他的处于standby状态(standby是instance的状态)。
- *canal client*: 为了保证有序性，一份instance同一时间只能由一个canal client进行get/ack/rollback操作，否则客户端接收无法保证有序。

server ha的架构图如下：

![canal的HA机制.jpg](images/bVbJUWV.jpeg)

大致步骤：

1. canal server要启动某个*canal instance*时都先向zookeeper_进行一次尝试启动判断_(实现：创建EPHEMERAL节点，谁创建成功就允许谁启动)
2. 创建zookeeper节点成功后，对应的canal server就启动对应的canal instance，*没有创建成功的canal instance就会处于standby状态*。
3. 一旦zookeeper发现canal server A创建的*instance节点*消失后，立即通知其他的canal server再次进行步骤1的操作，重新选出一个canal server启动instance。
4. canal client每次进行connect时，会首先向zookeeper询问当前是谁启动了canal instance，然后和其建立链接，一旦链接不可用，会重新尝试connect。

Canal Client的方式和canal server方式类似，也是利用zookeeper的抢占EPHEMERAL节点的方式进行控制.

### Kettle

### FileBeat

### Logstash

## 计算引擎

### 对比

#### Storm、Flink、Spark实时计算方案比较

**实时计算方案列表如下：**

| 计算引擎 | Storm                                       | Flink                                                        | spark-streaming                                              |
| :------- | :------------------------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| API      | 灵活的底层 API 和具有事务保证的 Trident API | 流 API 和更加适合数据开发的 Table API 和 Flink SQL 支持      | 流 API 和 Structured-Streaming API<br/>同时也可以使用更适合数据开发的 Spark SQL |
| 容错机制 | ACK 机制                                    | State 分布式快照保存点                                       | RDD 保存点                                                   |
| 状态管理 | Trident State状态管理                       | Key State 和 Operator State两种 State 可以使用，支持多种持久化方案 | 有 UpdateStateByKey 等 API 进行带状态的变更，支持多种持久化方案 |
| 处理模式 | 单条流式处理                                | 单条流式处理                                                 | Mic batch处理                                                |
| 延迟     | 毫秒级                                      | 毫秒级（事件驱动）                                           | 秒级（Micro-batch）                                          |
| 语义保障 | At Least Once，Exactly Once                 | Exactly Once，At Least Once                                  | At Least Once                                                |

从调研结果来看，Flink 和 Spark Streaming 的 API 、容错机制与状态持久化机制都可以解决一部分我们目前使用 Storm 中遇到的问题。但 Flink 在数据延迟上和 Storm 更接近，对现有应用影响最小。而且在公司内部的测试中 Flink 的吞吐性能对比 Storm 有十倍左右提升。综合考量可以选定 Flink 引擎作为实时数仓的开发引擎。

划重点：**Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。**

### Flink

#### Flink部署方式

- Local—本地单机模式，学习测试时使用

> ![image-20210722125318092.png](images/b376cecf098644e6aed4f615d73bab0a~tplv-k3u1fbpfcp-watermark.awebp)
>
> 1. Flink程序由JobClient进行提交
> 2. JobClient将作业提交给JobManager
> 3. JobManager负责协调资源分配和作业执行。资源分配完成后，任务将提交给相应的TaskManager
> 4. TaskManager启动一个线程以开始执行。TaskManager会向JobManager报告状态更改,如开始执行，正在进行或已完成。
> 5. 作业执行完成后，结果将发送回客户端(JobClient)

- Standalone—独立集群模式，Flink自带集群，开发测试环境使用

  > ![image-20210722125536908.png](images/ce10d15fa69545f99996ed77040b247e~tplv-k3u1fbpfcp-watermark.awebp)
  >
  > 1. client客户端提交任务给JobManager
  > 2. JobManager负责申请任务运行所需要的资源并管理任务和资源
  > 3. JobManager分发任务给TaskManager执行
  > 4. TaskManager定期向JobManager汇报状态

- StandaloneHA—独立集群高可用模式，Flink自带集群，开发测试环境使用

  > ![image-20210722125704208.png](images/0dec0068a43949a485031d3fc3039d1b~tplv-k3u1fbpfcp-watermark.awebp)
  >
  > 从之前的架构中我们可以很明显的发现 JobManager 有明显的单点问题(SPOF，single point of failure)。JobManager 肩负着任务调度以及资源分配，一旦 JobManager 出现意外，其后果可想而知。
  >
  > 在 Zookeeper 的帮助下，一个 Standalone的Flink集群会同时有多个活着的 JobManager，其中只有一个处于工作状态，其他处于 Standby 状态。当工作中的 JobManager 失去连接后(如宕机或 Crash)，Zookeeper 会从 Standby 中选一个新的 JobManager 来接管 Flink 集群。

- On Yarn—计算资源统一由Hadoop YARN管理，生产环境使用

  > 为什么使用Flink On Yarn？
  >
  > 1. Yarn的资源可以按需使用，提高集群的资源利用率
  > 2. Yarn的任务有优先级，根据优先级运行作业
  > 3. 基于Yarn调度系统，能够自动化地处理各个角色的 Failover(容错)
  >    1. JobManager 进程和 TaskManager 进程都由 Yarn NodeManager 监控
  >    2. 如果 JobManager 进程异常退出，则 Yarn ResourceManager 会重新调度 JobManager 到其他机器
  >    3. 如果 TaskManager 进程异常退出，JobManager 会收到消息并重新向 Yarn ResourceManager 申请资源，重新启动 TaskManager
  >
  > **Flink如何和YARN进行交互？**
  >
  > ![image-20210722125956080.png](images/7494e8c387894fdaa9a4636f80037154~tplv-k3u1fbpfcp-watermark.awebp)
  >
  > ![image-20210722130001000.png](images/fef57e3e7cf544cf9190aa38eae21c75~tplv-k3u1fbpfcp-watermark.awebp)
  >
  > 1. Client上传jar包和配置文件到HDFS集群上
  >
  > 2. Client向Yarn ResourceManager提交任务并申请资源
  >
  > 3. .ResourceManager分配Container资源并启动ApplicationMaster,然后AppMaster加载Flink的Jar包和配置构建环境,启动JobManager
  >
  >    1. JobManager和ApplicationMaster运行在同一个container上。
  >
  >    2. 一旦他们被成功启动，AppMaster就知道JobManager的地址(AM它自己所在的机器)。
  >
  >    3. 它就会为TaskManager生成一个新的Flink配置文件(他们就可以连接到JobManager)。这个配置文件也被上传到HDFS上。
  >
  >    4. 此外，AppMaster容器也提供了Flink的web服务接口。
  >
  >       YARN所分配的所有端口都是临时端口，这允许用户并行执行多个Flink
  >
  > 4. ApplicationMaster向ResourceManager申请工作资源,NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager
  >
  > 5. TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务

#### 四大基石

Checkpoint

> 这是Flink最重要的一个特性。
>
> Flink基于Chandy-Lamport算法实现了一个分布式的一致性的快照，从而提供了一致性的语义。
>
> Chandy-Lamport算法实际上在1985年的时候已经被提出来，但并没有被很广泛的应用，而Flink则把这个算法发扬光大了。
>
> Spark最近在实现Continue streaming，Continue streaming的目的是为了降低处理的延时，其也需要提供这种一致性的语义，最终也采用了Chandy-Lamport这个算法，说明Chandy-Lamport算法在业界得到了一定的肯定。

State

> 提供了一致性的语义之后，Flink为了让用户在编程时能够更轻松、更容易地去管理状态，还提供了一套非常简单明了的State API，包括里面的有ValueState、ListState、MapState，近期添加了BroadcastState，使用State API能够自动享受到这种一致性的语义。

Time

> 除此之外，Flink还实现了Watermark的机制，能够支持基于事件的时间的处理，能够容忍迟到/乱序的数据。

Window

> 另外流计算中一般在对流数据进行操作之前都会先进行开窗，即基于一个什么样的窗口上做这个计算。Flink提供了开箱即用的各种窗口，比如滑动窗口、滚动窗口、会话窗口以及非常灵活的自定义的窗口。

#### 说说Flink的状态后端

- State backend 的选择

  > | StateBackend        | in-flight     | checkpoint | 吞吐 | 推荐使用场景                             |
  > | :------------------ | :------------ | :--------- | :--- | :--------------------------------------- |
  > | MemoryStateBackend  | TM Memory     | JM Memory  | 高   | 调试、小状态或对数据丢失或重复无要求     |
  > | FsStateBackend      | TM Memory     | FS/HDFS    | 高   | 普通状态、窗口、KV 结构、高可用          |
  > | RocksDBStateBackend | RocksDB on TM | FS/HDFS    | 低   | 超大状态、超长窗口、大型 KV 结构、高可用 |

#### 基于Flink的双流JOIN方案

#### Flink如何确保Exactly-once语义

#### 理解基于Flink自定义状态管理

#### 如何自定义Source、Transformation、Sink算子

#### 什么是CEP，如何自定义CEP规则

#### Flink如何管理内存

- 堆内存管理（积极的内存管理）

  > Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上，这个内存块叫做 `MemorySegment`，它代表了一段固定长度的内存（默认大小为 32KB），也是 Flink 中最小的内存分配单元，并且提供了非常高效的读写方法。你可以把 MemorySegment 想象成是为 Flink 定制的 `java.nio.ByteBuffer`。它的底层可以是一个普通的 Java 字节数组（`byte[]`），也可以是一个申请在堆外的 `ByteBuffer`。每条记录都会以序列化的形式存储在一个或多个`MemorySegment`中。Flink 中的 Worker 名叫 TaskManager，是用来运行用户代码的 JVM 进程。TaskManager 的堆内存主要被分成了三个部分：
  >
  > <img src="images/TB17qs5JpXXXXXhXpXXXXXXXXXX-7897318.png" alt="img" style="zoom:67%;" />
  >
  > - **Network Buffers:** 一定数量的32KB大小的 buffer，主要用于数据的网络传输。在 TaskManager 启动的时候就会分配。默认数量是 2048 个，可以通过 `taskmanager.network.numberOfBuffers` 来配置。（阅读[这篇文章](http://wuchong.me/blog/2016/04/26/flink-internals-how-to-handle-backpressure/#网络传输中的内存管理)了解更多Network Buffer的管理）
  > - **Memory Manager Pool:** 这是一个由 `MemoryManager` 管理的，由众多`MemorySegment`组成的超大集合。Flink 中的算法（如 sort/shuffle/join）会向这个内存池申请 MemorySegment，将序列化后的数据存于其中，使用完后释放回内存池。默认情况下，池子占了堆内存的 70% 的大小。
  > - **Remaining (Free) Heap:** 这部分的内存是留给用户代码以及 TaskManager 的数据结构使用的。因为这些数据结构一般都很小，所以基本上这些内存都是给用户代码使用的。从GC的角度来看，可以把这里看成的新生代，也就是说这里主要都是由用户代码生成的短期对象。
  >
  > **注意：Memory Manager Pool 主要在Batch模式下使用。在Steaming模式下，该池子不会预分配内存，也不会向该池子请求内存块。也就是说该部分的内存都是可以给用户代码使用的。不过社区是打算在 Streaming 模式下也能将该池子利用起来。**
  >
  > Flink 采用类似 DBMS 的 sort 和 join 算法，直接操作二进制数据，从而使序列化/反序列化带来的开销达到最小。所以 Flink 的内部实现更像 C/C++ 而非 Java。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。如果要操作多块MemorySegment就像操作一块大的连续内存一样，Flink会使用逻辑视图（`AbstractPagedInputView`）来方便操作。下图描述了 Flink 如何存储序列化后的数据到内存块中，以及在需要的时候如何将数据存储到磁盘上。
  >
  > 从上面我们能够得出 Flink 积极的内存管理以及直接操作二进制数据有以下几点好处：
  >
  > 1. **减少GC压力。**显而易见，因为所有常驻型数据都以二进制的形式存在 Flink 的`MemoryManager`中，这些`MemorySegment`一直呆在老年代而不会被GC回收。其他的数据对象基本上是由用户代码生成的短生命周期对象，这部分对象可以被 Minor GC 快速回收。只要用户不去创建大量类似缓存的常驻型对象，那么老年代的大小是不会变的，Major GC也就永远不会发生。从而有效地降低了垃圾回收的压力。另外，这里的内存块还可以是堆外内存，这可以使得 JVM 内存更小，从而加速垃圾回收。
  > 2. **避免了OOM。**所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。在内存吃紧的情况下，算法（sort/join等）会高效地将一大批内存块写到磁盘，之后再读回来。因此，`OutOfMemoryErrors`可以有效地被避免。
  > 3. **节省内存空间。**Java 对象在存储上有很多额外的消耗（如上一节所谈）。如果只存储实际数据的二进制内容，就可以避免这部分消耗。
  > 4. **高效的二进制操作 & 缓存友好的计算。**二进制数据以定义好的格式存储，可以高效地比较与操作。另外，该二进制形式可以把相关的值，以及hash值，键值和指针等相邻地放进内存中。这使得数据结构可以对高速缓存更友好，可以从 L1/L2/L3 缓存获得性能的提升（下文会详细解释）。

- Flink 基于堆内存的内存管理机制已经可以解决很多JVM现存问题了，为什么还要引入堆外内存？

  > 1. 启动超大内存（上百GB）的JVM需要很长时间，GC停留时间也会很长（分钟级）。使用堆外内存的话，可以极大地减小堆内存（只需要分配Remaining Heap那一块），使得 TaskManager 扩展到上百GB内存不是问题。
  > 2. 高效的 IO 操作。堆外内存在写磁盘或网络传输时是 zero-copy，而堆内存的话，至少需要 copy 一次。
  > 3. 堆外内存是进程间共享的。也就是说，即使JVM进程崩溃也不会丢失数据。这可以用来做故障恢复（Flink暂时没有利用起这个，不过未来很可能会去做）。
  >
  > 但是强大的东西总是会有其负面的一面，不然为何大家不都用堆外内存呢。
  >
  > 1. 堆内存的使用、监控、调试都要简单很多。堆外内存意味着更复杂更麻烦。
  > 2. Flink 有时需要分配短生命周期的 `MemorySegment`，这个申请在堆上会更廉价。
  > 3. 有些操作在堆内存上会快一点点。
  >
  > Flink用通过`ByteBuffer.allocateDirect(numBytes)`来申请堆外内存，用 `sun.misc.Unsafe` 来操作堆外内存。

#### 布隆过滤器整合State编程解决state过大的问题

#### 使用Flink异步IO提升数据处理的性能

#### Flink中的Environment

Flink有以下几种Environment

1. 批处理Environment，ExecutionEnvironment

```
ExecutionEnvironment env= ExecutionEnvironment.getExecutionEnvironment();
```

2.流处理Environment，StreamExecutionEnvironment

```
StreamExecutionEnvironment env= StreamExecutionEnvironment.getExecutionEnvironment();
```

3. 本机Environment，LocalEnvironment

```
ExecutionEnvironment env= LocalEnvironment.getExecutionEnvironment();
```

4. java集合Environment，CollectionEnvironment

```
ExecutionEnvironment env = CollectionEnvironment.getExecutionEnvironment();
```

创建Environment的方法

1. getExecutionEnvironment ，含义就是本地运行就是 createLocalEnvironment，如果是通过client提交到集群上，就返回集群的环境

```
Creates an execution environment that represents the context ``in` `which` `the program is currently executed.``  ``* If the program is invoked standalone, this method returns a ``local` `execution environment, as returned by``  ``* {@link ``#createLocalEnvironment()}. If the program is invoked from within the command line client to be``  ``* submitted to a cluster, this method returns the execution environment of this cluster.
```

2. createLocalEnvironment ，返回本地执行环境，需要在调用时指定默认的并行度，比如

```
LocalStreamEnvironment env1 = StreamExecutionEnvironment.createLocalEnvironment(1);
LocalEnvironment env2 = ExecutionEnvironment.createLocalEnvironment(1);
```

3. createRemoteEnvironment， 返回集群执行环境，将 Jar 提交到远程服务器。需要在调用时指定 JobManager 的 IP 和端口号，并指定要在集群中运行的 Jar 包，比如

```
StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("127.0.0.1", 8080, "/path/word_count.jar");
ExecutionEnvironment env2 = ExecutionEnvironment.createRemoteEnvironment("127.0.0.1", 8080, "/path/word_count.jar");
```

#### DataStream API

Flink中的DataStream任务用于实现data streams的转换，data stream可以来自不同的数据源，比如消息队列，socket，文件等。

使用DataStream API需要使用stream env。

### Spark Streaming

### Kafka Streams

### Storm

### Spark

### MapReduce



## 其它

### Zeppelin

Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala, Python, R and more.

### Apache Calcite

### 场景题

#### 1亿个正整数,范围是0-42亿。求出现次数是2的数字，空间复杂度

#### 有一个IP地址库，假设有几十万条ip，如何判断某个ip地址是否在这个库中？

思路1：Bitmap：将每一条ip对应位图中的一个位，2^32次方(42亿多)个数据只需要512M空间。可以实现O(1)的查询复杂度

思路2：布隆过滤器

#### 在一个文件中有 10G 个整数,乱序排列,要求找出中位数(内存限制为 2G)

平衡二叉树，二叉堆？内存不足时两侧同时丢弃相同数量元素，平衡后的roott节点为中位数

#### 一个5T的文件，里面全是id，1-10^9 ，如何计算不同id的个数？

#### 海量日志数据，提取出某日访问百度次数最多的那个IP

#### 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词

#### 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数

#### 怎么在海量数据中找出重复次数最多的一个？

#### 100w个数中找出最大的100个数

1. 最小堆，找最大100个数
2. 快速排序
3. 选取前100个元素，排序，然后扫描剩余的元素，与排好序的元素中最小的相比，如果比它大，替换，重排前面，这跟堆排序思路一样

#### 40亿个不重复无序的unsigned int的整数，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

思路1：位图，40亿个不重复的数，每个数用1bit表示，出现或不出现，40\*10^8*1 = 0.5G大小。遍历这40亿个数，如果出现将对应位置为1，对于给定的数直接判断位图中对应的值。

思路2：将每个整数都看成32位的二进制数，从最高位，依次按位来分，按最高位0，1分成两个文件，每个文件数字个数小于20亿，与所要判断的数的最高为进行比较，从而知道去哪个文件继续比较，然后对于选定的文件再按照次高位比较再分成2个文件，再比较判断数对应的位数，依次循环，直到最后一位，就可以找到或判断没有该数了。时间复杂度O（log2n）,因为每次都将数据减少一半，直到最后一个。

#### 5亿个整数找他们的中位数

中位数的定义：一个给定排序好的序列，奇数个的话，我们就取中间的一个；偶数个的话，我们一般取中间两个数的平均值；因此对于本题，我们需得到中间的第50亿和第50亿+1这两个数；

#### 给定a、b两个文件各存50亿个url（每个占64字节）内存限制是4G，让你找出a、b文件共同的url

思路：每个文件的大小5G*64 = 32G，远远大于内存，需要对a，b分别分成小文件

利用一个hash(url)00，分别将a，b文件分别映射成1000个小文件，因为通过相同的映射函数，所以对于a，b，相同的url都在对应的文件中，（a0 vs b0, a1 vs b1等等）分别比对这1000个对应的小文件，可以通过先将a映射到一个hash表中，然后依次遍历b，判断是否在a中出现，出现过则说明重复。

#### ForkJoin实现大型浮点数组排序





